# -*- coding: utf-8 -*-
"""gpt2_with_mask_tasks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kFvJ79ddfU9JzgPBWpogktdu2oivxmkG
"""

#!pip install transformers

import argparse
from dataclasses import dataclass
import json
import logging
import os
import random
import sys
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

import transformers
#import wandb
from tqdm.auto import tqdm, trange


logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s: %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)

def get_data(file_path, sample, num_choices):
    """ Reads jsonl file (download links in readme) """
    data_file = open(file_path, "r")
    logger.info("Reading QA instances from jsonl dataset at: %s", file_path)
    item_jsons = []
    item_ids = []
    questions = []
    choice_lists = []
    answer_ids = []
    for line in data_file:
        item_jsons.append(json.loads(line.strip()))

    if sample != -1:
        item_jsons = random.sample(item_jsons, sample)
        logger.info("Sampling %d examples", sample)

    for item_json in tqdm(item_jsons,total=len(item_jsons)):
        item_id = item_json["id"]

        question_text = item_json["question"]["stem"]

        choice_label_to_id = {}
        choice_text_list = []
        choice_context_list = []
        choice_label_list = []
        choice_annotations_list = []

        any_correct = False
        choice_id_correction = 0

        for choice_id, choice_item in enumerate(item_json["question"]["choices"]):
            choice_label = choice_item["label"]
            choice_label_to_id[choice_label] = choice_id - choice_id_correction
            choice_text = choice_item["text"]

            choice_text_list.append(choice_text)
            choice_label_list.append(choice_label)

            if item_json.get('answerKey') == choice_label:
                if any_correct:
                    raise ValueError("More than one correct answer found for {item_json}!")
                any_correct = True


        if not any_correct and 'answerKey' in item_json:
            raise ValueError("No correct answer found for {item_json}!")


        answer_id = choice_label_to_id.get(item_json.get("answerKey"))
        # Pad choices with empty strings if not right number
        if len(choice_text_list) != num_choices:
            choice_text_list = (choice_text_list + num_choices * [''])[:num_choices]
            choice_context_list = (choice_context_list + num_choices * [None])[:num_choices]
            if answer_id is not None and answer_id >= num_choices:
                logging.warning(f"Skipping question with more than {num_choices} answers: {item_json}")
                continue

        item_ids.append(item_id)
        questions.append(question_text)
        choice_lists.append(choice_text_list)
        answer_ids.append(answer_id)

    data_file.close()
    return questions, choice_lists, answer_ids

@dataclass
class CustomArguments(transformers.TrainingArguments):
    sample_train: int = 0
    sample_eval: int = 0
    num_choices: int = 0
    model_name_or_path: str = "asdf"  # this is no longer a TrainingArgument attribute
        
    # python dataclasses cannot have positional attributes in subclass,
    # so give all attributes defaults and then make sure they are changed
    def __post_init__(self):
        if not (self.sample_train * self.sample_eval * self.num_choices) or \
               self.model_name_or_path == "asdf":  # make sure none are still default value
            raise TypeError("__init__ missing required argument(s)")

            
# "bert-base-uncased", 
# "bert-large-uncased-whole-word-masking", 
# "roberta-large",
# "distilbert-base-uncased", 
# "facebook/bart-large",
# "albert-large-v1",
# "google/electra-large-discriminator",
def get_args():
    """ Set hyperparameters """
    args = CustomArguments(
        output_dir="checkpoint",
        model_name_or_path="gpt2-medium",
        overwrite_output_dir=True,
        do_train=False,  # Zero shot
        do_eval=True,
        per_device_eval_batch_size=8,
        learning_rate=1e-5,  # Should not matter because not training
        weight_decay=0.1,
        save_total_limit=2,
        seed=123,
        sample_train=200,
        sample_eval=-1,
        num_choices=5,
    )
    
    return args

class BERTDataset(Dataset):  # Only difference is that BERTDataset has token_type_ids while RoBERTaDataset doesn't
    
    def __init__(self, questions, choices, answer_ids, tokenizer):
        out = tokenizer(questions)
        self.input_ids = out["input_ids"]
        self.token_type_ids = out["token_type_ids"]
        self.attention_mask = out["attention_mask"]
        self.questions = questions
        self.choices = choices
        self.answer_ids = answer_ids
        
    def __len__(self):
        return len(self.questions)

    def __getitem__(self, i):
        return {
            "input_ids": self.input_ids[i], 
            "attention_mask": self.attention_mask[i], 
            "token_type_ids": self.token_type_ids[i],
            "choice_list": self.choices[i], 
            "answer_id": self.answer_ids[i],
        }
    

class RoBERTaDataset(Dataset):
    
    def __init__(self, questions, choices, answer_ids, tokenizer):
        if any(prefix in args.model_name_or_path.lower() for prefix in ("roberta", "bart")):
            questions = [question.replace('[MASK]','<mask>') for question in questions]
        out = tokenizer(questions, max_length=20, padding="max_length")
        self.input_ids = out["input_ids"]
        self.attention_mask = out["attention_mask"]
        self.questions = questions
        self.choices = choices
        self.answer_ids = answer_ids
        
    def __len__(self):
        return len(self.questions)

    def __getitem__(self, i):
        return {
            "input_ids": self.input_ids[i], 
            "attention_mask": self.attention_mask[i], 
            "choice_list": self.choices[i], 
            "answer_id": self.answer_ids[i],
        }

def get_sentence_prob(input_ids, logits):
    # Multiplies together individual probabilities to get overall sentence probability
    logits = torch.nn.functional.softmax(logits, dim=2)
  
    probs = torch.gather(logits, 2, input_ids.unsqueeze(-1)).squeeze(-1)
    probs = probs * 1e4  # product is zero otherwise
    probs = torch.prod(probs, dim=1)
    return probs

def evaluate_withmask(args, model, tokenizer, eval_dataset):
    """ 
    Evaluates model on the dataset, is currently hardcoded to only work for the Age Comparison task 
    `huggingface_generic_task.ipynb` works for all tasks but does not support GPT2
    """
    eval_sampler = SequentialSampler(eval_dataset)
    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.per_device_eval_batch_size)

    logger.info(f"***** Running evaluation  *****")
    logger.info(f"  Num examples = {len(eval_dataset)}")
    logger.info(f"  Batch size = {args.eval_batch_size}")
    eval_dataloader = tqdm(eval_dataloader, desc="Evaluating")
    
    #encoding all the labels
    label_dict = {}
    label_encodings = {}
    list_of_labels = eval_dataset[0]['choice_list']
    i=0
    for label in list_of_labels:
      label_dict[label] = i
      label1 = " "+label
      label_encodings[label1] = tokenizer.encode(label1, add_special_tokens=False)[0]
      i+=1
    
    all_answers = []
    all_preds = []
    
    #create list of true answers =  all_answers 
    for batch in eval_dataloader:
        original_batch = batch          
        model.eval()
        for i in range(len(batch["answer_id"])):
            true_label_id = batch["answer_id"][i]
            actual_label = batch["choice_list"][true_label_id][i]
            label_id_to_append = label_dict[actual_label]
            all_answers.append(label_id_to_append)

        del batch["choice_list"] 
        for key in batch:
            if key != "answer_id":
                batch[key] = torch.stack(batch[key], dim=-1)

            batch[key] = batch[key].cuda()
      
        
        answer_ids = batch.pop("answer_id")
        label_encoding_list = list(label_encodings.values())
        no_of_labels = len(label_encoding_list)
        with torch.no_grad():
            #generate probablities for all the labels
            
            list_of_mask_index = []
            for i in range(len(batch["input_ids"])):
                  question = batch["input_ids"][i]
                  MASK_INDEX = (question==tokenizer.mask_token_id).nonzero().item()
                  batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[0]
                  list_of_mask_index.append(MASK_INDEX)
            
            outputs = model(**batch)
            logits = outputs.logits
            id0_prob = get_sentence_prob(batch["input_ids"], logits)

            m=0
            for i in range(len(batch["input_ids"])):
                question = batch["input_ids"][i]
                MASK_INDEX = list_of_mask_index[m]
                batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[1]
                m +=1
           
            outputs = model(**batch)
            logits = outputs.logits
            id1_prob = get_sentence_prob(batch["input_ids"], logits)
            
            if no_of_labels==3:
              m=0
              for i in range(len(batch["input_ids"])):
                  question = batch["input_ids"][i]
                  MASK_INDEX = list_of_mask_index[m]
                  batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[2]
                  m +=1
             
              outputs = model(**batch)
              logits = outputs.logits
              id2_prob = get_sentence_prob(batch["input_ids"], logits)

            if no_of_labels==5:
                m=0
                for i in range(len(batch["input_ids"])):
                    question = batch["input_ids"][i]
                    MASK_INDEX = list_of_mask_index[m]
                    batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[2]
                    m +=1
                
                outputs = model(**batch)
                logits = outputs.logits
                id2_prob = get_sentence_prob(batch["input_ids"], logits)

                m=0
                for i in range(len(batch["input_ids"])):
                    question = batch["input_ids"][i]
                    MASK_INDEX = list_of_mask_index[m]
                    batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[3]
                    m +=1
                outputs = model(**batch)
                logits = outputs.logits
                id3_prob = get_sentence_prob(batch["input_ids"], logits)

                m=0
                for i in range(len(batch["input_ids"])):
                    question = batch["input_ids"][i]
                    MASK_INDEX = list_of_mask_index[m]
                    batch["input_ids"][i, MASK_INDEX] =  label_encoding_list[4]
                    m +=1
                outputs = model(**batch)
                logits = outputs.logits
                id4_prob = get_sentence_prob(batch["input_ids"], logits)
  
        batch_size = len(batch["input_ids"])
        #create all_preds
        if no_of_labels ==2:
          id0_prob = torch.reshape(id0_prob, (batch_size, 1))
          id1_prob = torch.reshape(id1_prob, (batch_size, 1))
          combine_prob = torch.cat((id0_prob, id1_prob), dim=1)
          preds = list(torch.argmax(combine_prob, dim=1))
          all_preds.extend(preds)
        elif no_of_labels ==3:
          id0_prob = torch.reshape(id0_prob, (batch_size, 1))
          id1_prob = torch.reshape(id1_prob, (batch_size, 1))
          id2_prob = torch.reshape(id2_prob, (batch_size, 1))
          combine_prob = torch.cat((id0_prob, id1_prob, id2_prob), dim=1)
          preds = list(torch.argmax(combine_prob, dim=1))
          all_preds.extend(preds)
        elif no_of_labels ==5:
          id0_prob = torch.reshape(id0_prob, (batch_size, 1))
          id1_prob = torch.reshape(id1_prob,(batch_size, 1))
          id2_prob = torch.reshape(id2_prob, (batch_size, 1))
          id3_prob = torch.reshape(id3_prob, (batch_size, 1))
          id4_prob = torch.reshape(id4_prob, (batch_size, 1))
          combine_prob = torch.cat((id0_prob, id1_prob, id2_prob, id3_prob, id4_prob), dim=1)
          preds = list(torch.argmax(combine_prob, dim=1))
          all_preds.extend(preds)
        
    return all_answers, all_preds

args = get_args()
transformers.set_seed(args.seed)

args.num_choices = 3
args.model_name_or_path = 'gpt2'
data = "data/compositional_comparison_dev.jsonl"

model = transformers.AutoModelWithLMHead.from_pretrained(args.model_name_or_path).cuda()
tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name_or_path , mask_token = '[MASK]')
tokenizer.pad_token = tokenizer.eos_token
#train_questions, train_choices, train_answer_ids = get_data(, args.sample_train, args.num_choices)
eval_questions, eval_choices, eval_answer_ids = get_data(data, args.sample_eval, args.num_choices)

AgeDataset = RoBERTaDataset if any(prefix in args.model_name_or_path.lower() for prefix in ("roberta", "bart", "distil", "gpt")) else BERTDataset
#train_dataset = AgeDataset(train_questions, train_choices, train_answer_ids, tokenizer)
eval_dataset = AgeDataset(eval_questions, eval_choices, eval_answer_ids, tokenizer)

#eval_dataset[12]

tokenizer.mask_token_id

all_answers, all_preds = evaluate_withmask(args, model, tokenizer, eval_dataset)

#results
print("lenght of all_answers = ",len(all_answers))
print("lenght of all_preds = ",len(all_preds))


all_pred = []
for i in all_preds:
    all_pred.append(i)

print("first 10 enteries of all_answers = ", all_answers[:10])
print("first 10 enteries of all_preds = ", all_preds[:10])

total = 0
for i in range(len(all_answers)):
  if all_answers[i]==all_pred[i]:
    total +=1

accuracy = total/len(all_answers) 
print("The accuracy is of {} for {} task is ".format(args.model_name_or_path, data), accuracy)



